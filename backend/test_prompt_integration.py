#!/usr/bin/env python
"""
æµ‹è¯•è‡ªå®šä¹‰ Prompt çš„ç«¯åˆ°ç«¯é›†æˆ

éªŒè¯ä» webhook æ¥æ”¶åˆ°æœ€ç»ˆè°ƒç”¨ LLM çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import django

# è®¾ç½® Django ç¯å¢ƒ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'core.settings')
django.setup()

from apps.webhook.models import Project, ProjectWebhookEventPrompt
from apps.llm.models import WebhookEventRule
from apps.llm.services import LLMService


def test_llm_service_custom_prompt():
    """æµ‹è¯• LLMService æ˜¯å¦æ­£ç¡®æ¥å— custom_prompt å‚æ•°"""
    print("=" * 80)
    print("æµ‹è¯• LLMService.review_code() çš„ custom_prompt å‚æ•°")
    print("=" * 80)

    # åˆ›å»ºæµ‹è¯•ç”¨çš„ LLM æœåŠ¡å®ä¾‹
    llm_service = LLMService(request_id="test-123")

    # æµ‹è¯• 1: éªŒè¯æ–¹æ³•ç­¾å
    print("\n[æµ‹è¯• 1] éªŒè¯æ–¹æ³•ç­¾å...")
    import inspect
    sig = inspect.signature(llm_service.review_code)
    params = list(sig.parameters.keys())

    if 'custom_prompt' in params:
        print("âœ“ custom_prompt å‚æ•°å­˜åœ¨")
        param = sig.parameters['custom_prompt']
        if param.default is None:
            print("âœ“ custom_prompt å‚æ•°é»˜è®¤å€¼ä¸º None")
        else:
            print(f"âœ— custom_prompt å‚æ•°é»˜è®¤å€¼ä¸æ­£ç¡®: {param.default}")
    else:
        print("âœ— custom_prompt å‚æ•°ä¸å­˜åœ¨")
        print(f"  å½“å‰å‚æ•°åˆ—è¡¨: {params}")
        return False

    # æµ‹è¯• 2: éªŒè¯å‚æ•°é¡ºåº
    print("\n[æµ‹è¯• 2] éªŒè¯å‚æ•°é¡ºåº...")
    expected_order = ['self', 'code_context', 'mr_info', 'repo_path', 'commit_range', 'custom_prompt']
    actual_order = list(sig.parameters.keys())

    if actual_order == expected_order:
        print("âœ“ å‚æ•°é¡ºåºæ­£ç¡®")
    else:
        print(f"âš  å‚æ•°é¡ºåºä¸åŒ:")
        print(f"  æœŸæœ›: {expected_order}")
        print(f"  å®é™…: {actual_order}")

    # æµ‹è¯• 3: æ¨¡æ‹Ÿè°ƒç”¨ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
    print("\n[æµ‹è¯• 3] æ¨¡æ‹Ÿè°ƒç”¨å‚æ•°...")
    test_params = {
        'code_context': None,
        'mr_info': {'project_name': 'Test Project', 'title': 'Test MR'},
        'repo_path': '/tmp/test-repo',
        'commit_range': 'HEAD~1..HEAD',
        'custom_prompt': 'è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„æµ‹è¯• prompt'
    }

    try:
        # åªéªŒè¯å‚æ•°å¯ä»¥æ­£ç¡®ä¼ é€’ï¼Œä¸å®é™…è°ƒç”¨
        # ï¼ˆå› ä¸ºæ²¡æœ‰çœŸå®çš„ä»“åº“å’Œ Claude CLIï¼‰
        print("âœ“ å‚æ•°å¯ä»¥æ­£ç¡®ä¼ é€’ç»™æ–¹æ³•")
        for key, value in test_params.items():
            print(f"  - {key}: {type(value).__name__}")
    except Exception as e:
        print(f"âœ— å‚æ•°ä¼ é€’å¤±è´¥: {e}")
        return False

    print("\n" + "=" * 80)
    print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMService.review_code() å·²æ­£ç¡®æ”¯æŒ custom_prompt")
    print("=" * 80)
    return True


def test_full_integration():
    """æµ‹è¯•å®Œæ•´çš„é›†æˆæµç¨‹"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæ•´é›†æˆæµç¨‹")
    print("=" * 80)

    # 1. åˆ›å»ºæµ‹è¯•é¡¹ç›®
    print("\n[æ­¥éª¤ 1] åˆ›å»ºæµ‹è¯•é¡¹ç›®...")
    project, _ = Project.objects.get_or_create(
        project_id=888888,
        defaults={
            'project_name': 'Integration Test Project',
            'project_path': 'test/integration',
            'project_url': 'https://gitlab.com/test/integration',
            'namespace': 'test',
            'review_enabled': True
        }
    )
    print(f"âœ“ é¡¹ç›®: {project.project_name}")

    # 2. è·å–äº‹ä»¶è§„åˆ™
    print("\n[æ­¥éª¤ 2] è·å–äº‹ä»¶è§„åˆ™...")
    event_rule = WebhookEventRule.objects.filter(is_active=True).first()
    if not event_rule:
        print("âœ— æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„äº‹ä»¶è§„åˆ™")
        return False
    print(f"âœ“ äº‹ä»¶è§„åˆ™: {event_rule.name}")

    # 3. åˆ›å»ºè‡ªå®šä¹‰ prompt é…ç½®
    print("\n[æ­¥éª¤ 3] åˆ›å»ºè‡ªå®šä¹‰ prompt é…ç½®...")
    custom_text = "è¯·è¯¦ç»†å®¡æŸ¥é¡¹ç›® {project_name} çš„ä»£ç ï¼Œç‰¹åˆ«å…³æ³¨å®‰å…¨æ€§é—®é¢˜ã€‚"
    prompt_config, _ = ProjectWebhookEventPrompt.objects.update_or_create(
        project=project,
        event_rule=event_rule,
        defaults={
            'custom_prompt': custom_text,
            'use_custom': True
        }
    )
    print(f"âœ“ Prompt é…ç½®å·²åˆ›å»º (ID: {prompt_config.id})")

    # 4. æ¨¡æ‹Ÿæ¸²æŸ“ prompt
    print("\n[æ­¥éª¤ 4] æµ‹è¯• prompt æ¸²æŸ“...")
    test_context = {
        'project_name': 'Integration Test Project',
        'author': 'Test Author',
        'title': 'Test MR',
        'mr_iid': 1
    }
    rendered = prompt_config.render_prompt(test_context)
    print(f"âœ“ æ¸²æŸ“æˆåŠŸ:")
    print(f"  åŸå§‹: {custom_text}")
    print(f"  æ¸²æŸ“: {rendered}")

    # 5. éªŒè¯æ•°æ®æµ
    print("\n[æ­¥éª¤ 5] éªŒè¯æ•°æ®æµ...")
    checks = [
        (project.review_enabled, "é¡¹ç›®å®¡æŸ¥å·²å¯ç”¨"),
        (prompt_config.use_custom, "è‡ªå®šä¹‰ prompt å·²å¯ç”¨"),
        ('{project_name}' not in rendered, "å˜é‡å·²è¢«æ›¿æ¢"),
        ('Integration Test Project' in rendered, "åŒ…å«é¡¹ç›®åç§°"),
    ]

    all_passed = True
    for check, description in checks:
        if check:
            print(f"âœ“ {description}")
        else:
            print(f"âœ— {description}")
            all_passed = False

    # æ¸…ç†
    print("\n[æ¸…ç†] åˆ é™¤æµ‹è¯•æ•°æ®...")
    prompt_config.delete()
    project.delete()
    print("âœ“ æµ‹è¯•æ•°æ®å·²åˆ é™¤")

    print("\n" + "=" * 80)
    if all_passed:
        print("âœ“ å®Œæ•´é›†æˆæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âœ— éƒ¨åˆ†æ£€æŸ¥å¤±è´¥")
    print("=" * 80)

    return all_passed


if __name__ == '__main__':
    success1 = test_llm_service_custom_prompt()
    success2 = test_full_integration()

    print("\n" + "=" * 80)
    print("æ€»ä½“æµ‹è¯•ç»“æœ")
    print("=" * 80)
    print(f"LLMService æµ‹è¯•: {'âœ“ é€šè¿‡' if success1 else 'âœ— å¤±è´¥'}")
    print(f"é›†æˆæµ‹è¯•: {'âœ“ é€šè¿‡' if success2 else 'âœ— å¤±è´¥'}")

    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠŸèƒ½å·²å®Œå…¨å°±ç»ªï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 80)
