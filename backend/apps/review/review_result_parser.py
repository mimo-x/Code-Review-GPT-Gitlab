"""
Review Result Parser for Claude CLI Output
Parses Claude CLI JSON output and formats it for storage and display
"""
import re
import logging
from typing import Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


class ReviewResultParser:
    """
    Parser for Claude CLI review results
    """

    def __init__(self, request_id=None):
        self.request_id = request_id

    def parse(self, claude_output: Dict) -> Dict:
        """
        è§£æ Claude CLI çš„è¾“å‡ºç»“æœ

        Args:
            claude_output: Claude CLI è¿”å›çš„ JSON æ•°æ®

        Returns:
            è§£æåçš„å®¡æŸ¥æ•°æ®å­—å…¸:
            {
                'content': 'æ ¼å¼åŒ–çš„å®¡æŸ¥å†…å®¹',
                'score': 85,  # 0-100
                'duration_ms': 12345,
                'token_usage': {...},
                'issues': [...],
                'summary': '...',
                'metadata': {...}
            }
        """
        try:
            result_text = claude_output.get('result', '')

            # æå–åŸºæœ¬ä¿¡æ¯
            parsed_data = {
                'content': result_text,
                'raw_result': claude_output,
                'duration_ms': claude_output.get('duration_ms', 0),
                'duration_api_ms': claude_output.get('duration_api_ms', 0),
                'num_turns': claude_output.get('num_turns', 0),
                'token_usage': claude_output.get('usage', {}),
                'model_usage': claude_output.get('modelUsage', {}),
                'total_cost_usd': claude_output.get('total_cost_usd', 0.0),
            }

            # è§£æå®¡æŸ¥å†…å®¹
            parsed_data['issues'] = self._extract_issues(result_text)
            parsed_data['score'] = self._calculate_score(result_text, parsed_data['issues'])
            parsed_data['summary'] = self._extract_summary(result_text)
            parsed_data['security_issues'] = self._extract_security_issues(result_text)
            parsed_data['performance_issues'] = self._extract_performance_issues(result_text)

            # æ„å»ºå…ƒæ•°æ®
            parsed_data['metadata'] = {
                'score': parsed_data['score'],
                'total_issues': len(parsed_data['issues']),
                'critical_issues': len([i for i in parsed_data['issues'] if i.get('severity') == 'critical']),
                'security_issues': len(parsed_data['security_issues']),
                'performance_issues': len(parsed_data['performance_issues']),
                'duration_seconds': parsed_data['duration_ms'] / 1000,
                'total_cost': parsed_data['total_cost_usd'],
            }

            logger.info(f"[{self.request_id}] Parsed review result: {parsed_data['metadata']}")

            return parsed_data

        except Exception as e:
            logger.error(f"[{self.request_id}] Error parsing Claude output: {e}", exc_info=True)
            # è¿”å›åŸºæœ¬ç»“æ„ä»¥é¿å…å´©æºƒ
            return {
                'content': claude_output.get('result', 'Error parsing result'),
                'score': 0,
                'issues': [],
                'summary': 'Error parsing review result',
                'metadata': {},
                'raw_result': claude_output,
            }

    def _extract_issues(self, text: str) -> List[Dict]:
        """
        ä»å®¡æŸ¥æ–‡æœ¬ä¸­æå–é—®é¢˜åˆ—è¡¨

        Args:
            text: å®¡æŸ¥æ–‡æœ¬

        Returns:
            é—®é¢˜åˆ—è¡¨
        """
        issues = []

        try:
            # æŸ¥æ‰¾æ ‡è®°ä¸ºä¸¥é‡ã€é«˜å±ã€ä¸­å±ç­‰çº§åˆ«çš„é—®é¢˜
            severity_patterns = [
                (r'ğŸ”´\s*ä¸¥é‡|ä¸¥é‡å®‰å…¨é£é™©|Critical', 'critical'),
                (r'ğŸŸ \s*é«˜å±|High', 'high'),
                (r'ğŸŸ¡\s*ä¸­å±|æ¬¡è¦|Medium', 'medium'),
                (r'ğŸŸ¢\s*ä½å±|å»ºè®®|Low', 'low'),
            ]

            # æŒ‰è¡Œåˆ†æ
            lines = text.split('\n')
            current_issue = None
            current_severity = 'medium'

            for line in lines:
                stripped = line.strip()

                # æ£€æµ‹ä¸¥é‡æ€§çº§åˆ«
                for pattern, severity in severity_patterns:
                    if re.search(pattern, stripped, re.IGNORECASE):
                        current_severity = severity
                        break

                # æ£€æµ‹é—®é¢˜æ ‡é¢˜ï¼ˆé€šå¸¸ä»¥ **æ•°å­—.** æˆ– ### å¼€å¤´ï¼‰
                title_match = re.match(r'^[#*]+\s*\d+[\.ã€]\s*(.+)|^\*\*(\d+)\.\s*(.+)\*\*', stripped)
                if title_match:
                    if current_issue:
                        issues.append(current_issue)

                    title = title_match.group(1) or title_match.group(3) or stripped
                    current_issue = {
                        'title': title.strip('*# '),
                        'description': '',
                        'severity': current_severity,
                        'file': None,
                        'line': None,
                    }

                # æå–æ–‡ä»¶å’Œè¡Œå·ä¿¡æ¯
                file_match = re.search(r'`?([a-zA-Z0-9_/\.\-]+\.py|\.js|\.vue|\.java|\.go):(\d+)', stripped)
                if file_match and current_issue:
                    current_issue['file'] = file_match.group(1)
                    current_issue['line'] = int(file_match.group(2))

                # æ·»åŠ æè¿°è¡Œ
                if current_issue and stripped and not title_match:
                    if current_issue['description']:
                        current_issue['description'] += '\n'
                    current_issue['description'] += stripped

            # æ·»åŠ æœ€åä¸€ä¸ªé—®é¢˜
            if current_issue:
                issues.append(current_issue)

            logger.info(f"[{self.request_id}] Extracted {len(issues)} issues from review")

        except Exception as e:
            logger.error(f"[{self.request_id}] Error extracting issues: {e}")

        return issues

    def _calculate_score(self, text: str, issues: List[Dict]) -> int:
        """
        æ ¹æ®å®¡æŸ¥å†…å®¹è®¡ç®—è¯„åˆ†

        Args:
            text: å®¡æŸ¥æ–‡æœ¬
            issues: é—®é¢˜åˆ—è¡¨

        Returns:
            è¯„åˆ† (0-100)
        """
        try:
            # åŸºç¡€åˆ†æ•°
            score = 100

            # æ ¹æ®é—®é¢˜æ•°é‡å’Œä¸¥é‡ç¨‹åº¦æ‰£åˆ†
            for issue in issues:
                severity = issue.get('severity', 'medium')
                if severity == 'critical':
                    score -= 20
                elif severity == 'high':
                    score -= 10
                elif severity == 'medium':
                    score -= 5
                elif severity == 'low':
                    score -= 2

            # æŸ¥æ‰¾æ˜ç¡®çš„è¯„åˆ†æ ‡è®°
            score_match = re.search(r'è¯„åˆ†[ï¼š:]\s*(\d+)', text)
            if score_match:
                explicit_score = int(score_match.group(1))
                # ä½¿ç”¨æ˜¾å¼è¯„åˆ†å’Œè®¡ç®—è¯„åˆ†çš„å¹³å‡å€¼
                score = (score + explicit_score) // 2

            # ç¡®ä¿åˆ†æ•°åœ¨ 0-100 èŒƒå›´å†…
            score = max(0, min(100, score))

            logger.info(f"[{self.request_id}] Calculated review score: {score}")

            return score

        except Exception as e:
            logger.error(f"[{self.request_id}] Error calculating score: {e}")
            return 70  # é»˜è®¤åˆ†æ•°

    def _extract_summary(self, text: str) -> str:
        """
        æå–å®¡æŸ¥æ‘˜è¦

        Args:
            text: å®¡æŸ¥æ–‡æœ¬

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        try:
            # æŸ¥æ‰¾æ‘˜è¦éƒ¨åˆ†
            summary_patterns = [
                r'##\s*æ‘˜è¦\s*\n(.+?)(?=\n##|\n\n|\Z)',
                r'##\s*Summary\s*\n(.+?)(?=\n##|\n\n|\Z)',
                r'##\s*æ€»ç»“\s*\n(.+?)(?=\n##|\n\n|\Z)',
            ]

            for pattern in summary_patterns:
                match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
                if match:
                    summary = match.group(1).strip()
                    logger.info(f"[{self.request_id}] Extracted summary: {summary[:100]}...")
                    return summary

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ‘˜è¦éƒ¨åˆ†ï¼Œä½¿ç”¨å‰200å­—ç¬¦
            lines = text.split('\n')
            summary_lines = []
            for line in lines[:10]:  # å–å‰10è¡Œ
                if line.strip() and not line.strip().startswith('#'):
                    summary_lines.append(line.strip())
                if len(' '.join(summary_lines)) > 200:
                    break

            summary = ' '.join(summary_lines)[:200]
            return summary if summary else 'ä»£ç å®¡æŸ¥å·²å®Œæˆ'

        except Exception as e:
            logger.error(f"[{self.request_id}] Error extracting summary: {e}")
            return 'ä»£ç å®¡æŸ¥å·²å®Œæˆ'

    def _extract_security_issues(self, text: str) -> List[Dict]:
        """
        æå–å®‰å…¨ç›¸å…³é—®é¢˜

        Args:
            text: å®¡æŸ¥æ–‡æœ¬

        Returns:
            å®‰å…¨é—®é¢˜åˆ—è¡¨
        """
        security_issues = []

        try:
            # å®‰å…¨å…³é”®è¯
            security_keywords = [
                'SQLæ³¨å…¥', 'XSS', 'CSRF', 'å‘½ä»¤æ³¨å…¥', 'è·¯å¾„éå†',
                'API Key', 'Token', 'å¯†ç ', 'Secret', 'ç¡¬ç¼–ç ',
                'è®¤è¯', 'æˆæƒ', 'æƒé™', 'åŠ å¯†', 'æ˜æ–‡'
            ]

            lines = text.split('\n')
            for i, line in enumerate(lines):
                for keyword in security_keywords:
                    if keyword.lower() in line.lower():
                        # è·å–ä¸Šä¸‹æ–‡
                        context_start = max(0, i - 1)
                        context_end = min(len(lines), i + 2)
                        context = '\n'.join(lines[context_start:context_end])

                        security_issues.append({
                            'keyword': keyword,
                            'line_number': i + 1,
                            'context': context,
                        })
                        break  # æ¯è¡Œåªè®°å½•ä¸€æ¬¡

            logger.info(f"[{self.request_id}] Found {len(security_issues)} security-related mentions")

        except Exception as e:
            logger.error(f"[{self.request_id}] Error extracting security issues: {e}")

        return security_issues

    def _extract_performance_issues(self, text: str) -> List[Dict]:
        """
        æå–æ€§èƒ½ç›¸å…³é—®é¢˜

        Args:
            text: å®¡æŸ¥æ–‡æœ¬

        Returns:
            æ€§èƒ½é—®é¢˜åˆ—è¡¨
        """
        performance_issues = []

        try:
            # æ€§èƒ½å…³é”®è¯
            performance_keywords = [
                'N+1', 'æ€§èƒ½', 'ä¼˜åŒ–', 'ç¼“å­˜', 'ç´¢å¼•',
                'å¤æ‚åº¦', 'å†…å­˜æ³„æ¼', 'å¹¶å‘', 'å¼‚æ­¥'
            ]

            lines = text.split('\n')
            for i, line in enumerate(lines):
                for keyword in performance_keywords:
                    if keyword.lower() in line.lower():
                        # è·å–ä¸Šä¸‹æ–‡
                        context_start = max(0, i - 1)
                        context_end = min(len(lines), i + 2)
                        context = '\n'.join(lines[context_start:context_end])

                        performance_issues.append({
                            'keyword': keyword,
                            'line_number': i + 1,
                            'context': context,
                        })
                        break

            logger.info(f"[{self.request_id}] Found {len(performance_issues)} performance-related mentions")

        except Exception as e:
            logger.error(f"[{self.request_id}] Error extracting performance issues: {e}")

        return performance_issues

    def format_for_report(self, parsed_data: Dict) -> str:
        """
        å°†è§£æåçš„æ•°æ®æ ¼å¼åŒ–ä¸ºæŠ¥å‘Šæ–‡æœ¬

        Args:
            parsed_data: è§£æåçš„æ•°æ®

        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
        """
        try:
            report_parts = []

            # æ ‡é¢˜å’Œæ‘˜è¦
            report_parts.append(f"# ä»£ç å®¡æŸ¥æŠ¥å‘Š\n")
            report_parts.append(f"**è¯„åˆ†**: {parsed_data['score']}/100\n")
            report_parts.append(f"**é—®é¢˜æ€»æ•°**: {len(parsed_data['issues'])}\n")
            report_parts.append(f"**å®¡æŸ¥è€—æ—¶**: {parsed_data['duration_ms'] / 1000:.2f}ç§’\n")
            report_parts.append(f"\n## æ‘˜è¦\n{parsed_data['summary']}\n")

            # ä¸»è¦é—®é¢˜
            if parsed_data['issues']:
                report_parts.append(f"\n## å‘ç°çš„é—®é¢˜\n")
                for i, issue in enumerate(parsed_data['issues'], 1):
                    severity_emoji = {
                        'critical': 'ğŸ”´',
                        'high': 'ğŸŸ ',
                        'medium': 'ğŸŸ¡',
                        'low': 'ğŸŸ¢'
                    }.get(issue['severity'], 'âšª')

                    report_parts.append(f"\n### {severity_emoji} {i}. {issue['title']}\n")

                    if issue.get('file'):
                        report_parts.append(f"**æ–‡ä»¶**: `{issue['file']}`")
                        if issue.get('line'):
                            report_parts.append(f" (è¡Œ {issue['line']})")
                        report_parts.append('\n')

                    if issue.get('description'):
                        report_parts.append(f"{issue['description']}\n")

            # åŸå§‹å†…å®¹
            report_parts.append(f"\n## è¯¦ç»†åˆ†æ\n{parsed_data['content']}\n")

            # å…ƒæ•°æ®
            report_parts.append(f"\n---\n")
            report_parts.append(f"*å®¡æŸ¥ç”± Claude AI å®Œæˆï¼Œè€—æ—¶ {parsed_data['duration_ms']}ms*\n")

            return '\n'.join(report_parts)

        except Exception as e:
            logger.error(f"[{self.request_id}] Error formatting report: {e}")
            return parsed_data.get('content', 'Error formatting report')
